
# 安装Slurm相关包
install_slurm_packages() {
    log_step "安装Slurm相关软件包"
    
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        if [ "$node" == "master" ]; then
            log_info "在 $node ($ip) 上安装主节点Slurm包..."
            ssh root@"$ip" "
            dnf install -y ohpc-slurm-server slurm-ohpc slurm-devel-ohpc slurm-example-configs-ohpc slurm-slurmctld-ohpc slurm-slurmdbd-ohpc slurm-slurmd-ohpc mariadb-server mariadb
            echo '主节点Slurm包安装完成'
            "
        else
            log_info "在 $node ($ip) 上安装计算节点Slurm包..."
            ssh root@"$ip" "
            dnf install -y ohpc-slurm-client slurm-ohpc slurm-slurmd-ohpc
            echo '计算节点Slurm包安装完成'
            "
        fi
    done
}


# 配置Munge
setup_munge() {
    log_step "配置Munge认证"
    
    # 在所有节点安装munge
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        ssh root@"$ip" "
        if ! dnf list installed munge &>/dev/null; then
            dnf install -y munge munge-libs munge-devel
            echo 'munge 已安装'
        else
            echo 'munge 已安装，跳过'
        fi
        
        # 确保munge用户存在
        if ! id munge &>/dev/null; then
            useradd -r -s /sbin/nologin munge 2>/dev/null || true
            echo '已创建munge用户'
        fi
        "
    done
    
    # 在master节点生成密钥
    master_ip="${node_ips[0]}"
    ssh root@"$master_ip" "
    if [ ! -f /etc/munge/munge.key ]; then
        /usr/sbin/create-munge-key -r
        echo '已生成Munge密钥'
    else
        echo 'Munge密钥已存在，跳过'
    fi
    "
    
    # 将密钥分发到所有节点
    log_info "分发Munge密钥到所有节点"
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        if [ "$node" != "master" ]; then
            scp root@"$master_ip":/etc/munge/munge.key root@"$ip":/etc/munge/munge.key
            ssh root@"$ip" "chown munge:munge /etc/munge/munge.key && chmod 400 /etc/munge/munge.key"
            log_info "已将Munge密钥分发到 $node"
        else
            ssh root@"$ip" "chown munge:munge /etc/munge/munge.key && chmod 400 /etc/munge/munge.key"
            log_info "已设置 $node 上的Munge密钥权限"
        fi
    done
    
    # 启动munge服务
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        ssh root@"$ip" "
        # 确保munge用户存在后再启动服务
        if ! id munge &>/dev/null; then
            useradd -r -s /sbin/nologin munge 2>/dev/null || true
        fi
        
        systemctl enable munge
        systemctl start munge
        echo 'Munge服务已启动'
        "
    done
    
    log_info "Munge配置完成"
}


# 配置Slurm
configure_slurm() {
    log_step "配置Slurm"
    
    # 获取硬件信息
    get_node_hardware_info
    
    # 在所有节点创建Slurm用户和目录
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        ssh root@"$ip" "
        # 创建Slurm用户
        if ! id slurm &>/dev/null; then
            useradd -r -s /bin/false -d /var/lib/slurm slurm
            echo '已创建slurm用户'
        else
            echo 'slurm用户已存在，跳过'
        fi
        
        # 创建必要目录
        mkdir -p /var/spool/slurm/ctld
        mkdir -p /var/spool/slurm/d
        mkdir -p /var/log/slurm
        mkdir -p /etc/slurm  # 添加这行确保配置目录存在
        
        chown slurm:slurm /var/spool/slurm/ctld
        chown slurm:slurm /var/spool/slurm/d
        chown slurm:slurm /var/log/slurm
        "
    done
    
    # 在master节点生成配置文件
    master_ip="${node_ips[0]}"
    master_hostname="${node_hostnames[0]}"
    
    # 构建节点配置部分
    node_configs=""
    partition_nodes=""
    
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        hostname="${node_hostnames[$i]}"
        cpus="${node_cpus[$i]}"
        memory="${node_memory[$i]}"
        
        # 保留90%的内存给Slurm使用
        slurm_memory=$((memory * 90 / 100))
        
        node_configs+="NodeName=$hostname CPUs=$cpus RealMemory=$slurm_memory State=UNKNOWN\n"
        if [ -n "$partition_nodes" ]; then
            partition_nodes+=",$hostname"
        else
            partition_nodes="$hostname"
        fi
    done
    
    # 生成slurm.conf
    ssh root@"$master_ip" "
    cat > /etc/slurm/slurm.conf << 'EOF'
# slurm.conf file generated by automated script
ClusterName=cluster
ControlMachine=$master_hostname
ControlAddr=$master_hostname

SlurmUser=slurm
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmdPidFile=/var/run/slurm/slurmd.pid
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
MaxJobCount=10000
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core

# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/slurmdbd
AccountingStoreFlags=job_comment
JobCompType=jobcomp/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log

# NODES
$(echo -e "$node_configs")

# PARTITIONS
PartitionName=compute Nodes=$partition_nodes Default=YES MaxTime=INFINITE State=UP
EOF
    "
    
    # 生成slurmdbd.conf
    ssh root@"$master_ip" "
    cat > /etc/slurm/slurmdbd.conf << EOF
AuthType=auth/munge
AuthInfo=/var/run/munge/munge.socket.2
DbdAddr=localhost
DbdHost=localhost
SlurmUser=slurm
DebugLevel=verbose
LogFile=/var/log/slurm/slurmdbd.log
PidFile=/var/run/slurm/slurmdbd.pid
StorageType=accounting_storage/mysql
StorageHost=localhost
StoragePass=$slurm_db_pass
StorageUser=slurm
StorageLoc=slurm_acct_db
EOF
    
    chmod 600 /etc/slurm/slurmdbd.conf
    chown slurm:slurm /etc/slurm/slurmdbd.conf
    "
    
    # 将配置文件分发到所有计算节点
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        hostname="${node_hostnames[$i]}"
        
        if [ "$node" != "master" ]; then
            # 确保目标节点上的目录存在后再分发配置文件
            ssh root@"$ip" "mkdir -p /etc/slurm"
            scp root@"$master_ip":/etc/slurm/slurm.conf root@"$ip":/etc/slurm/slurm.conf
            log_info "已将slurm.conf分发到 $node"
        fi
    done
    
    log_info "Slurm配置完成"
}


# 启动Slurm服务
start_slurm_services() {
    log_step "启动Slurm服务"
    
    # 在master节点启动服务
    master_ip="${node_ips[0]}"
    ssh root@"$master_ip" "
    systemctl enable slurmdbd
    systemctl enable slurmctld
    
    systemctl start slurmdbd
    sleep 5
    systemctl start slurmctld
    
    echo '主节点Slurm服务已启动'
    "
    
    # 在所有节点启动slurmd服务
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        ssh root@"$ip" "
        systemctl enable slurmd
        systemctl start slurmd
        echo '$node 节点Slurm服务已启动'
        "
    done
    
    log_info "所有Slurm服务已启动"
}


# 验证安装
verify_installation() {
    log_step "验证安装"
    
    master_ip="${node_ips[0]}"
    
    # 检查slurm命令是否存在
    log_info "检查Slurm命令是否存在"
    ssh root@"$master_ip" "
    if ! command -v sinfo &> /dev/null; then
        echo '错误: sinfo命令未找到'
        exit 1
    fi
    
    if ! command -v sbatch &> /dev/null; then
        echo '错误: sbatch命令未找到'
        exit 1
    fi
    
    if ! command -v squeue &> /dev/null; then
        echo '错误: squeue命令未找到'
        exit 1
    fi
    
    if ! command -v scontrol &> /dev/null; then
        echo '错误: scontrol命令未找到'
        exit 1
    fi
    "
    
    # 检查服务状态
    log_info "检查Slurm服务状态"
    ssh root@"$master_ip" "
    if ! systemctl is-active --quiet slurmctld; then
        echo '错误: slurmctld服务未运行'
        systemctl status slurmctld
        exit 1
    fi
    
    if ! systemctl is-active --quiet slurmdbd; then
        echo '错误: slurmdbd服务未运行'
        systemctl status slurmdbd
        exit 1
    fi
    "
    
    # 在所有计算节点检查slurmd服务
    for i in "${!nodes[@]}"; do
        node="${nodes[$i]}"
        ip="${node_ips[$i]}"
        
        ssh root@"$ip" "
        if ! systemctl is-active --quiet slurmd; then
            echo '错误: slurmd服务在 $node 节点未运行'
            systemctl status slurmd
            exit 1
        fi
        "
    done
    
    # 等待服务完全启动
    log_info "等待服务启动"
    sleep 15
    
    # 检查节点状态
    log_info "检查集群节点状态"
    ssh root@"$master_ip" "
    node_status=\$(sinfo -h -o '%T' | head -1)
    if [[ -z \"\$node_status\" ]]; then
        echo '错误: 无法获取节点状态'
        exit 1
    fi
    echo '集群节点状态: \$node_status'
    
    # 显示节点详情
    echo '=== 节点详情 ==='
    scontrol show nodes
    "
    
    log_info "安装验证完成"
}
